{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 5 - Soil maps etc. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A map is of the form\n",
    "XX YY ZZ\n",
    "\n",
    "Where:\n",
    "- XX is destination range start\n",
    "- YY is source range start\n",
    "- ZZ is range length\n",
    "\n",
    "Such that a Source-to-Destination map of 50 98 2 corresponds to:\n",
    "- Destination source = 50\n",
    "- Source range start = 98\n",
    "- Range length = 2\n",
    "And this means that we have the mapping Source:Destination of {98:50, 99:51}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./example.txt\") as f:\n",
    "    example_lines = []\n",
    "    for line in f.readlines():\n",
    "        example_lines.append(line.strip())\n",
    "\n",
    "with open(\"./input.txt\") as f:\n",
    "    input_lines = []\n",
    "    for line in f.readlines():\n",
    "        input_lines.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['seeds: 79 14 55 13',\n",
       " '',\n",
       " 'seed-to-soil map:',\n",
       " '50 98 2',\n",
       " '52 50 48',\n",
       " '',\n",
       " 'soil-to-fertilizer map:',\n",
       " '0 15 37',\n",
       " '37 52 2',\n",
       " '39 0 15']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_lines[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'maps': {'seed-to-soil': [{'destination_start': 50, 'source_start': 98, 'range': 2}, {'destination_start': 52, 'source_start': 50, 'range': 48}], 'soil-to-fertilizer': [{'destination_start': 0, 'source_start': 15, 'range': 37}, {'destination_start': 37, 'source_start': 52, 'range': 2}, {'destination_start': 39, 'source_start': 0, 'range': 15}], 'fertilizer-to-water': [{'destination_start': 49, 'source_start': 53, 'range': 8}, {'destination_start': 0, 'source_start': 11, 'range': 42}, {'destination_start': 42, 'source_start': 0, 'range': 7}, {'destination_start': 57, 'source_start': 7, 'range': 4}], 'water-to-light': [{'destination_start': 88, 'source_start': 18, 'range': 7}, {'destination_start': 18, 'source_start': 25, 'range': 70}], 'light-to-temperature': [{'destination_start': 45, 'source_start': 77, 'range': 23}, {'destination_start': 81, 'source_start': 45, 'range': 19}, {'destination_start': 68, 'source_start': 64, 'range': 13}], 'temperature-to-humidity': [{'destination_start': 0, 'source_start': 69, 'range': 1}, {'destination_start': 1, 'source_start': 0, 'range': 69}], 'humidity-to-location': [{'destination_start': 60, 'source_start': 56, 'range': 37}, {'destination_start': 56, 'source_start': 93, 'range': 4}]}, 'seeds': [79, 14, 55, 13]}\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEEDS = \"seeds\"\n",
    "MAPS = \"maps\"\n",
    "DESTINATION_START = \"destination_start\"\n",
    "SOURCE_START = \"source_start\"\n",
    "RANGE = \"range\"\n",
    "\n",
    "def parse_input(lines: list[str]) -> dict:\n",
    "    \"\"\"\n",
    "    Returns dictionary of the form:\n",
    "    {\n",
    "        \"seeds\": list[int],\n",
    "        \"maps\": {\n",
    "            \"X-to-Y\": [\n",
    "                {\n",
    "                    \"destination_start\": int,\n",
    "                    \"source_start\": int,\n",
    "                    \"range\": int,\n",
    "                },\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    \"\"\"\n",
    "    d = {}\n",
    "    d[MAPS] = {}\n",
    "\n",
    "    # Extract seeds\n",
    "    seed_line = lines[0].split(\": \")[-1]\n",
    "    seeds = [int(seed) for seed in seed_line.split()]\n",
    "    assert isinstance(seeds, list), isinstance(seeds[0], int)\n",
    "    d[SEEDS] = seeds\n",
    "\n",
    "    # Extract maps!\n",
    "    parsing_map = False\n",
    "    map_key = \"INVALID\"\n",
    "    for line in lines[1:]:\n",
    "        assert isinstance(line, str)\n",
    "        if line == \"\":\n",
    "            parsing_map = False\n",
    "            map_key = \"INVALID\"\n",
    "            continue\n",
    "        if parsing_map:\n",
    "            values = [int(val) for val in line.split()]\n",
    "            assert len(values) == 3\n",
    "            destination_start, source_start, map_range = values\n",
    "            tmp_mapping_list = d[MAPS][map_key]\n",
    "            assert isinstance(tmp_mapping_list, list)\n",
    "            tmp_mapping_list.append(\n",
    "                {\n",
    "                    DESTINATION_START: destination_start,\n",
    "                    SOURCE_START: source_start,\n",
    "                    RANGE: map_range,\n",
    "                }\n",
    "            )\n",
    "        if \"map\" in line:\n",
    "            map_key = line.split()[0]\n",
    "            assert map_key not in d[MAPS].keys()\n",
    "            d[MAPS][map_key] = []\n",
    "            parsing_map = True\n",
    "    \n",
    "    return d\n",
    "\n",
    "initial_example_mapping = parse_input(example_lines)\n",
    "initial_example_mapping.__str__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_on_input = parse_input(input_lines)  # Ok well that part is plenty quick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source_start = 2642418175 \n",
    "# destination_start = 2192252668\n",
    "# map_range = 507721065\n",
    "\n",
    "# new_map = dict(zip(\n",
    "#     range(source_start, source_start+map_range),\n",
    "#     range(destination_start, destination_start+map_range)\n",
    "# ))\n",
    "\n",
    "# Evidently this kind of approach is wayyyy too inefficient!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_map_func(initial_mapping: dict[str, list[dict[str, int]]]) -> callable:\n",
    "    \"\"\"\n",
    "    So we're taking a dict of the form:\n",
    "    {\n",
    "        \"source-to-destination\": [\n",
    "            {\n",
    "                \"destination_start\": int,\n",
    "                \"source_start\": int,\n",
    "                \"range\": int,\n",
    "            },\n",
    "            {\n",
    "                \"destination_start\": int,\n",
    "                \"source_start\": int,\n",
    "                \"range\": int,\n",
    "            },\n",
    "            ...\n",
    "        ], \n",
    "        ...\n",
    "    }\n",
    "\n",
    "    And converting to one of the form\n",
    "    \n",
    "    {\n",
    "        \"source-to-destination\": dict[source: int, destination: int]\n",
    "    } ***OLD***\n",
    "\n",
    "    {\n",
    "        \"source-to-destination\": func(val_to_map: int) -> int\n",
    "    } NEW\n",
    "    \"\"\"\n",
    "    # new_mapping_dict = {}\n",
    "\n",
    "    # DESTINATION, SOURCE, RANGE tuples\n",
    "    mapping_tuples_list = []\n",
    "\n",
    "    for single_map in initial_mapping:\n",
    "        assert isinstance(single_map, dict)\n",
    "        destination_start = single_map[DESTINATION_START]\n",
    "        source_start = single_map[SOURCE_START]\n",
    "        map_range = single_map[RANGE]\n",
    "\n",
    "        # new_map = {\n",
    "        #     (source_start + i):(destination_start + i)\n",
    "        #     for i in range(map_range)\n",
    "        # }  \n",
    "        # Above method took like a million years lol, left in for fun\n",
    "        # new_map = dict(zip(\n",
    "        #     range(source_start, source_start+map_range),\n",
    "        #     range(destination_start, destination_start+map_range)\n",
    "        # ))\n",
    "        # Ok that also was way too slow....\n",
    "        \n",
    "        # lets try tuples..., and functions??\n",
    "        mapping_tuples_list.append(\n",
    "            (destination_start, source_start, map_range)\n",
    "        )\n",
    "    \n",
    "    def f(value_to_map: int):\n",
    "        # find out which tuple said input would be applicable to:\n",
    "        for tuple_map in mapping_tuples_list:\n",
    "            destination, source, range_len = tuple_map\n",
    "            if source <= value_to_map <= source+range_len:\n",
    "                diff = destination - source\n",
    "                return value_to_map+diff\n",
    "\n",
    "        # didn't find\n",
    "        return value_to_map\n",
    "\n",
    "    return f\n",
    "    # return new_mapping_dict\n",
    "\n",
    "func = extract_map_func(initial_example_mapping[MAPS][\"seed-to-soil\"])\n",
    "assert func(79) == 81"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.extract_map_func.<locals>.f(value_to_map: int)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check it's not too slow on the input version!\n",
    "extract_map_func(test_on_input[MAPS][\"seed-to-soil\"])  # way quicker lol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_map_key(source_name: str, map_names: list[str]) -> tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Takes in source name e.g. \"seed\", and the list of map_names\n",
    "    Finds the map_name that corresponds in this example to \"seed-to-X\"\n",
    "    returns this mapping name alongisde the destination string \"X\"\n",
    "    \"\"\"\n",
    "    relevant_names = [name for name in map_names if source_name+\"-to-\" in name]\n",
    "    assert len(relevant_names) == 1\n",
    "    map_name = relevant_names[0]\n",
    "    destination_name = relevant_names[0].split(\"-\")[-1]\n",
    "\n",
    "    return map_name, destination_name"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 1: return the lowest location number that corresponds to any of the initial seeds**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "841 µs ± 19 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "def part1(lines: list[str]):\n",
    "    initial_mapping_dict = parse_input(lines)\n",
    "\n",
    "    initial_seeds = initial_mapping_dict[SEEDS]\n",
    "\n",
    "    maps = [map_name for map_name in initial_mapping_dict[MAPS].keys()]\n",
    "    final_mapping_dict = {}\n",
    "    for map_name in maps:\n",
    "        final_mapping_dict[map_name] = extract_map_func(initial_mapping_dict[MAPS][map_name])\n",
    "    \n",
    "    lowest_value = 999999999999999999999999999999999999\n",
    "    for seed in initial_seeds:\n",
    "        source_name = \"seed\"\n",
    "        location_found = False\n",
    "        current_value = seed\n",
    "        while not location_found:\n",
    "            map_name, destination_name = get_map_key(source_name, maps)\n",
    "            # If there's not a mapping, it maps to itself\n",
    "            next_value = final_mapping_dict[map_name](current_value)\n",
    "            if destination_name == \"location\":\n",
    "                location_found = True\n",
    "                lowest_value = min(lowest_value, next_value)\n",
    "            else:\n",
    "                source_name = destination_name\n",
    "                current_value = next_value\n",
    "    \n",
    "    return lowest_value\n",
    "\n",
    "assert part1(example_lines) == 35\n",
    "%timeit part1(input_lines)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A nice improvement from first attempt with the huge dictionary maps lol"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 2: just more seeds to parse**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> update- regretting saying 'just'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def part2_failed_attempt(lines: list[str]):\n",
    "    initial_mapping_dict = parse_input(lines)\n",
    "\n",
    "    \n",
    "    maps = [map_name for map_name in initial_mapping_dict[MAPS].keys()]\n",
    "    final_mapping_dict = {}\n",
    "    for map_name in maps:\n",
    "        final_mapping_dict[map_name] = extract_map_func(initial_mapping_dict[MAPS][map_name])\n",
    "\n",
    "    def search_seeds(seed: int):\n",
    "        source_name = \"seed\"\n",
    "        location_found = False\n",
    "        current_value = seed\n",
    "        while not location_found:\n",
    "            map_name, destination_name = get_map_key(source_name, maps)\n",
    "            # If there's not a mapping, it maps to itself\n",
    "            next_value = final_mapping_dict[map_name](current_value)\n",
    "            if destination_name == \"location\":\n",
    "                location_found = True\n",
    "                return next_value\n",
    "            else:\n",
    "                source_name = destination_name\n",
    "                current_value = next_value\n",
    "    \n",
    "    search_seeds_vectorised = np.vectorize(search_seeds, cache=True)\n",
    "\n",
    "    initial_seeds_raw = initial_mapping_dict[SEEDS]\n",
    "    starts_and_lengths = [(initial_seeds_raw[i], initial_seeds_raw[i+1]) for i in range(0, len(initial_seeds_raw), 2)]\n",
    "\n",
    "    input_s_l_vectors = [\n",
    "        np.arange(start, start+length) for start, length in starts_and_lengths\n",
    "    ]\n",
    "\n",
    "    output_list = Parallel(n_jobs=-1)(\n",
    "        delayed(search_seeds_vectorised)(input_vector)\n",
    "        for input_vector in input_s_l_vectors\n",
    "    )\n",
    "    # output_list = []\n",
    "    # for input_vector in input_s_l_vectors:\n",
    "    #     output_list.append(search_seeds_vectorised(input_vector))\n",
    "    #     print(\"done a vector\")\n",
    "\n",
    "    outputs = np.concatenate(output_list)\n",
    "    return outputs.min()\n",
    "\n",
    "assert part2_failed_attempt(example_lines) == 46"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"./medium_example.txt\") as f:\n",
    "#     medium_lines = []\n",
    "#     for line in f.readlines():\n",
    "#         medium_lines.append(line.strip())\n",
    "\n",
    "# part2_failed_attempt(medium_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def part2(lines: list[str]) -> int:\n",
    "    initial_mapping_dict = parse_input(lines)\n",
    "\n",
    "    maps = [map_name for map_name in initial_mapping_dict[MAPS].keys()]\n",
    "    final_mapping_dict = {}\n",
    "    for map_name in maps:\n",
    "        final_mapping_dict[map_name] = extract_map_func(initial_mapping_dict[MAPS][map_name])\n",
    "\n",
    "    def search_seeds(seed: int):\n",
    "        source_name = \"seed\"\n",
    "        location_found = False\n",
    "        current_value = seed\n",
    "        while not location_found:\n",
    "            map_name, destination_name = get_map_key(source_name, maps)\n",
    "            # If there's not a mapping, it maps to itself\n",
    "            next_value = final_mapping_dict[map_name](current_value)\n",
    "            if destination_name == \"location\":\n",
    "                location_found = True\n",
    "                return next_value\n",
    "            else:\n",
    "                source_name = destination_name\n",
    "                current_value = next_value\n",
    "    \n",
    "    search_seeds_vectorised = np.vectorize(search_seeds, cache=True)\n",
    "\n",
    "    initial_seeds_raw = initial_mapping_dict[SEEDS]\n",
    "    starts_and_lengths = [(initial_seeds_raw[i], initial_seeds_raw[i+1]) for i in range(0, len(initial_seeds_raw), 2)]\n",
    "\n",
    "    raw_input_s_l_vectors = [\n",
    "        np.arange(start, start+length) for start, length in starts_and_lengths\n",
    "    ]\n",
    "    # need to split the input_s_l_vectors into more manageable chunks?\n",
    "    input_s_l_vectors = []\n",
    "    for big_vector in raw_input_s_l_vectors:\n",
    "        # If the big vector is of size 1,000,000,000\n",
    "        # and we want all vectors to be of size 1,000,000\n",
    "        # we split into big_vector.size // 1,000,000 = 10 vectors\n",
    "        num_splits = max(1, big_vector.size // 100000.)\n",
    "        input_s_l_vectors.extend(\n",
    "            np.array_split(big_vector, num_splits)\n",
    "        )\n",
    "\n",
    "    output_list = Parallel(n_jobs=-1)(\n",
    "        delayed(search_seeds_vectorised)(input_vector)\n",
    "        for input_vector in input_s_l_vectors\n",
    "    )\n",
    "    # import time\n",
    "\n",
    "    # output_list = []\n",
    "    # print(len(input_s_l_vectors))\n",
    "    # for vec in input_s_l_vectors:\n",
    "        # t1 = time.time()\n",
    "        # output_list.append(search_seeds_vectorised(vec))\n",
    "        # t2 = time.time()\n",
    "        # print(f\"did a vec of size {vec.size} in {t2-t1}\")\n",
    "\n",
    "    outputs = np.concatenate(output_list)\n",
    "    return outputs.min()\n",
    "\n",
    "assert part2(example_lines) == 46"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"./medium_example.txt\") as f:\n",
    "#     medium_lines = []\n",
    "#     for line in f.readlines():\n",
    "#         medium_lines.append(line.strip())\n",
    "\n",
    "# part2(medium_lines)\n",
    "\n",
    "# Medium estimated to take 1656 * 6 seconds on one core so like 2.76 hours"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimated that part2(input_lines) would take 30 hours on one core, so hoping on all cores it's more like 4 hours? Trying overnight lol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "OverflowError",
     "evalue": "Python int too large to convert to C long",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"c:\\Users\\adiplock\\AppData\\Local\\mambaforge\\envs\\misc-practice\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 463, in _process_worker\n    r = call_item()\n        ^^^^^^^^^^^\n  File \"c:\\Users\\adiplock\\AppData\\Local\\mambaforge\\envs\\misc-practice\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 291, in __call__\n    return self.fn(*self.args, **self.kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\adiplock\\AppData\\Local\\mambaforge\\envs\\misc-practice\\Lib\\site-packages\\joblib\\parallel.py\", line 589, in __call__\n    return [func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\adiplock\\AppData\\Local\\mambaforge\\envs\\misc-practice\\Lib\\site-packages\\joblib\\parallel.py\", line 589, in <listcomp>\n    return [func(*args, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\adiplock\\AppData\\Local\\mambaforge\\envs\\misc-practice\\Lib\\site-packages\\numpy\\lib\\function_base.py\", line 2329, in __call__\n    return self._vectorize_call(func=func, args=vargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\adiplock\\AppData\\Local\\mambaforge\\envs\\misc-practice\\Lib\\site-packages\\numpy\\lib\\function_base.py\", line 2415, in _vectorize_call\n    res = asanyarray(outputs, dtype=otypes[0])\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nOverflowError: Python int too large to convert to C long\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[75], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m part2(input_lines)\n",
      "Cell \u001b[1;32mIn[74], line 43\u001b[0m, in \u001b[0;36mpart2\u001b[1;34m(lines)\u001b[0m\n\u001b[0;32m     38\u001b[0m     num_splits \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(\u001b[39m1\u001b[39m, big_vector\u001b[39m.\u001b[39msize \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m100000.\u001b[39m)\n\u001b[0;32m     39\u001b[0m     input_s_l_vectors\u001b[39m.\u001b[39mextend(\n\u001b[0;32m     40\u001b[0m         np\u001b[39m.\u001b[39marray_split(big_vector, num_splits)\n\u001b[0;32m     41\u001b[0m     )\n\u001b[1;32m---> 43\u001b[0m output_list \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)(\n\u001b[0;32m     44\u001b[0m     delayed(search_seeds_vectorised)(input_vector)\n\u001b[0;32m     45\u001b[0m     \u001b[39mfor\u001b[39;49;00m input_vector \u001b[39min\u001b[39;49;00m input_s_l_vectors\n\u001b[0;32m     46\u001b[0m )\n\u001b[0;32m     47\u001b[0m \u001b[39m# import time\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \n\u001b[0;32m     49\u001b[0m \u001b[39m# output_list = []\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[39m# t2 = time.time()\u001b[39;00m\n\u001b[0;32m     55\u001b[0m     \u001b[39m# print(f\"did a vec of size {vec.size} in {t2-t1}\")\u001b[39;00m\n\u001b[0;32m     57\u001b[0m outputs \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate(output_list)\n",
      "File \u001b[1;32mc:\\Users\\adiplock\\AppData\\Local\\mambaforge\\envs\\misc-practice\\Lib\\site-packages\\joblib\\parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1946\u001b[0m \u001b[39m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   1947\u001b[0m \u001b[39m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   1948\u001b[0m \u001b[39m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[0;32m   1949\u001b[0m \u001b[39m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   1950\u001b[0m \u001b[39mnext\u001b[39m(output)\n\u001b[1;32m-> 1952\u001b[0m \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39m(output)\n",
      "File \u001b[1;32mc:\\Users\\adiplock\\AppData\\Local\\mambaforge\\envs\\misc-practice\\Lib\\site-packages\\joblib\\parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1592\u001b[0m     \u001b[39myield\u001b[39;00m\n\u001b[0;32m   1594\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1595\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_retrieve()\n\u001b[0;32m   1597\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1598\u001b[0m     \u001b[39m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1599\u001b[0m     \u001b[39m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1600\u001b[0m     \u001b[39m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1601\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\adiplock\\AppData\\Local\\mambaforge\\envs\\misc-practice\\Lib\\site-packages\\joblib\\parallel.py:1699\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1692\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wait_retrieval():\n\u001b[0;32m   1693\u001b[0m \n\u001b[0;32m   1694\u001b[0m     \u001b[39m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[0;32m   1695\u001b[0m     \u001b[39m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[0;32m   1696\u001b[0m     \u001b[39m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[0;32m   1697\u001b[0m     \u001b[39m# worker traceback.\u001b[39;00m\n\u001b[0;32m   1698\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_aborting:\n\u001b[1;32m-> 1699\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raise_error_fast()\n\u001b[0;32m   1700\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m   1702\u001b[0m     \u001b[39m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1703\u001b[0m     \u001b[39m# async callbacks to progress.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\adiplock\\AppData\\Local\\mambaforge\\envs\\misc-practice\\Lib\\site-packages\\joblib\\parallel.py:1734\u001b[0m, in \u001b[0;36mParallel._raise_error_fast\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1730\u001b[0m \u001b[39m# If this error job exists, immediatly raise the error by\u001b[39;00m\n\u001b[0;32m   1731\u001b[0m \u001b[39m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[0;32m   1732\u001b[0m \u001b[39m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[0;32m   1733\u001b[0m \u001b[39mif\u001b[39;00m error_job \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1734\u001b[0m     error_job\u001b[39m.\u001b[39;49mget_result(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout)\n",
      "File \u001b[1;32mc:\\Users\\adiplock\\AppData\\Local\\mambaforge\\envs\\misc-practice\\Lib\\site-packages\\joblib\\parallel.py:736\u001b[0m, in \u001b[0;36mBatchCompletionCallBack.get_result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    730\u001b[0m backend \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparallel\u001b[39m.\u001b[39m_backend\n\u001b[0;32m    732\u001b[0m \u001b[39mif\u001b[39;00m backend\u001b[39m.\u001b[39msupports_retrieve_callback:\n\u001b[0;32m    733\u001b[0m     \u001b[39m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[0;32m    734\u001b[0m     \u001b[39m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[0;32m    735\u001b[0m     \u001b[39m# be returned.\u001b[39;00m\n\u001b[1;32m--> 736\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_return_or_raise()\n\u001b[0;32m    738\u001b[0m \u001b[39m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[0;32m    739\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\adiplock\\AppData\\Local\\mambaforge\\envs\\misc-practice\\Lib\\site-packages\\joblib\\parallel.py:754\u001b[0m, in \u001b[0;36mBatchCompletionCallBack._return_or_raise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    752\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    753\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstatus \u001b[39m==\u001b[39m TASK_ERROR:\n\u001b[1;32m--> 754\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result\n\u001b[0;32m    755\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result\n\u001b[0;32m    756\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "\u001b[1;31mOverflowError\u001b[0m: Python int too large to convert to C long"
     ]
    }
   ],
   "source": [
    "part2(input_lines)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RIP this attempt :("
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
